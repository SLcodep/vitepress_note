---
title: 中国铁塔证件识别项目
date: 2025-01-05 00:58:41
permalink: /qianduan/铁塔证件识别项目
categories:
  - 前端
tags:
  - 项目
---

::: info 项目介绍
**项目概述：** 团队受中国铁塔公司委托，搭建一套用于工作人员在线提交图片或者房产证件信息，对于上传的图片进行 p 图检测，对于上传的房产证件进行房产证信息识别与提取和相同房产证相似度匹配并且能够导出报告的在线检测系统。
**项目技术**：采用 Vue3.0+ElementPlus+Axios+Pinia+Xlsx 实现
亮点：1）实现用户管理与权限控制 2）实现前端文件夹上传功能解决文件批量上传痛点；3）利用 keep-alive 保存组件状态减少重复请求；4）应用路由懒加载优化性能；5）通过 XLSX 库实现表格导出功能
:::

## 1.前端权限控制方案
主要包括的内容是：路由控制、组件控制、权限管理、后端验证、缓存同步等多个环节。
前端权限控制的核心准则：
1.前端只控制界面展示，不能控制数据和行为
2.权限要后端返回、前端解析
3.所有权限判断必须同步做后端验证
所以前端权限控制主要是权限路由守卫、控制按钮和组件显示，目的是减少页面结构暴露、增强用户体验的功效。

**1.权限路由守卫**
通过 Vue Router 的导航守卫beforeEach() 来进行权限校验，拦截不符合权限的访问请求。

## 2.利用keep-alive保存组件状态减少重复请求

>在这个项目中，经常做的事是上传图片到服务器，通过算法将结果再返回给前端页面。之后如果切换到其他页面，返回时之前上传的图片和返回的检测结果都被刷新。用户体验极差，服务器也因为重复请求列表数据而压力倍增。

**生命周期的改变（关键点）**

**1.没有 keep-alive 时：**
- 进入：`created` -> `mounted`
- 离开：`destroyed`
- 再次进入：`created` -> `mounted` （API 请求通常放在这里，所以会重复跑）

**2.有 keep-alive 时：**
- 初次进入：`created` -> `mounted `-> `activated`
- 离开：`deactivated` （注意：没有 destroyed）
- 再次进入：`activated` （注意：没有 created 和 mounted）

```js
<template>
    <!-- 将（只）缓存组件name为a或者b的组件, 结合动态组件使用 -->
    <keep-alive include="a,b">
          <component :is="view"></component>
    </keep-alive>

    <!-- 组件name为c的组件不缓存(可以保留它的状态或避免重新渲染) -->
    <keep-alive exclude="c"> 
          <component :is="view"></component>
    </keep-alive>
    
    <!-- 使用正则表达式，需使用v-bind -->
    <keep-alive :include="/a|b/">
          <component :is="view"></component>
    </keep-alive>

    <!-- 动态判断 -->
    <keep-alive :include="includedComponents">
        <router-view></router-view>
    </keep-alive>
    
    <!-- 如果同时使用include,exclude,那么exclude优先于include， 下面的例子只缓存a组件 -->
    <keep-alive include="a,b" exclude="b"> 
          <component :is="view"></component>
    </keep-alive>

    <!-- 如果缓存的组件超过了max设定的值5，那么将删除第一个被缓存的组件 -->
    <keep-alive exclude="c" max="5"> 
      <component></component>
    </keep-alive>
</template>
```

## 3.路由懒加载优化
>懒加载解决了首屏慢的问题，但也带来了后续跳转慢的风险。所以我结合了 Prefetch（预获取）策略。对于核心路径（Critical Path）上的页面，我配置了 `webpackPrefetch: true`，让浏览器在网络空闲时默默下载。这样既保证了首屏速度，又实现了后续操作的‘秒开’。

首先明确路由懒加载的目的是显著提升首屏加载速度(FCP)。要思考几个问题,一个是这个路由拆分要如何划分？第二个问题就是在首屏加载完成后，为了之后跳转加载速度快，启动预获取。
```js
//预获取
component: () => import(
  /* webpackChunkName: "user", webpackPrefetch: true */ 
  '@/views/user/index.vue'
)
```

## 4.用户体验优化
针对**上传慢 + 算法慢**导致体验割裂的问题，对前端进行一定的优化。
1.图片质量压缩。OCR检测算法一般不太需要分辨率过高的图片，
2.分片上传 + 进度可视化。通过显示上传进度能是用户对时间判断更清晰。
3.多阶段反馈。
```text
✔ 图片上传完成
✔ 正在校验证件完整性
✔ 正在检测异常篡改区域
⏳ 正在生成检测结果
```
4.本地缓存+Hash去重。用户在检测时大量上传图片，在这其中容易有重复的图片。
```text
选图
 ↓
前端算 hash
 ↓
查本地缓存
 ↓
有结果 → 直接展示
 ↓
没结果 → 上传 → 存缓存
```
### 4.1 图片质量压缩
### 4.2 分片上传 + 进度可视化
### 4.3 多阶段反馈
### 4.4 本地缓存+Hash去重


## 5.文件上传
> 大文件上传中的切片上传、断点续传、秒传、暂停等功能,以及并行上传都是如何实现的？
:::tip
大文件上传可抽象为 6 个阶段：

1.文件选择（File API 获取文件对象）
2.分片 + 哈希计算（生成唯一标识）
3.文件校验（秒传 / 断点续传判断）
4.分片上传（含并发控制、错误重试、中断机制）
5.合并分片（服务端流式拼接）
6.上传完成校验（文件完整性验证、存储持久化）
:::
### 1.文件选择
在文件选择上直接使用选胜的`<input type="file">`,其是 HTML 中的一个表单元素，专门用来让用户选择本地文件（比如图片、文档等）。
```js
<input type="file" @change="handleUpload">
const handleUpload = (e) => {
  console.log(e.target.files[0]);
}
```

### 2.文件分片与哈希优化
#### 文件分片
主要针对大文件，需要将大文件分片，切割成小片段。分片的逻辑主要由客户端处理，客户端的File对象自带分片功能：`file.slice()`。

#### 哈希优化
而下一个问题就是如何区分向服务器上传文件时进行区分？文件名因为可以随便进行修改，无法进行内容的区分。解决办法就是为**文件内容生成唯一的hash值**。
那么如何计算文件的hash值？通过一个工具: `spark-md5`。
如何来决定哪些文件进行hash计算？在文件分片中将一个大文件分成若干小文件，如果所有的小文件都进行hash值计算会很耗时间。所以可以采取一下策略：
1.第一个和最后一个切片的内容全部参与计算
2.中间剩余的切片我们分别在前面、后面和中间取2个字节参与计算
```js
const calculateHash = (chunks)=>{
  const spark = new SparkMD5.ArrayBuffer();
  const reader = new FileReader();
  const targets = [];

  chunks.forEach((chunk,index)=>{
    if(index === 0 || index === chunks.length - 1) {
      targets.push(chunk);
    } else{
      targets.push(chunk.slice(0, 2));
      targets.push(chunk.slice(CHUNK_SIZE/2,CHUNK_SIZE/2 + 2));
      targets.push(chunk.slice(CHUNK_SIZE - 2, CHUNK_SIZE));
    }
  })
  return new Promise((resolve)=>{
    // 把所有采样片段合并成一个Blob，读取为ArrayBuffer
    reader.readAsArrayBuffer(new Blob(targets));
    // 读取完成后计算MD5并返回结果
    reader.onload = (e)=>{
      spark.append(e.target.result); // 将ArrayBuffer数据加入MD5计算
      resolve(spark.end());          // 完成计算，返回最终的MD5哈希字符串
      console.log(spark.end());      // 打印最终的MD5哈希字符串
    }
  })
}
```
这里有一个问题，为什么要返回为Promise对象
`calculateHash` 函数内部使用了 `FileReader API`的 `readAsArrayBuffer` 方法来读取文件内容。FileReader 是一个 异步API ，它不会立即返回结果，而是在文件读取完成后通过 onload 事件回调来通知结果。
### 3.校验文件状态（秒传/断点续传）
> chrome浏览器来说，默认的并发数量是6。上传文件时一般要用到`FormData`对象。`FormData`能解决二进制文件的传输格式 + 数据拼接 + 前后端兼容这几个核心问题。
**功能说明**：
- 向后端发送文件哈希值和文件名
- 获取文件在服务器上的状态（是否存在、已上传哪些分片）
- 决定后续上传策略（秒传、断点续传或正常上传）

```js
async function verifyFile(fileHash, fileName) {
  const res = await fetch("http://localhost:3000/verify", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ fileHash, fileName }),
  });
  return res.json();
}
```
### 4.分片上传
**逻辑讲解**：
1. 初始化已上传分片计数和进度
2. 遍历所有分片，为每个分片创建上传任务
3. 检查分片是否已上传，已上传的分片直接跳过
4. 为未上传的分片创建FormData对象，包含分片数据和相关信息
5. 使用Fetch API上传分片，并传递中断信号
6. 上传成功后更新已上传计数和进度
7. 使用Promise.all()并行处理所有上传任务

```js
/**
 * 分片上传核心函数（支持断点续传、上传进度、上传中断）
 * @param {Array} chunks - 前端拆分后的文件分片数组（每个元素是Blob/File类型的分片）
 * @param {String} fileHash - 整个文件的唯一哈希值（如MD5，用于后端标识文件）
 * @param {Array} existChunks - 后端返回的已上传分片列表（断点续传用，默认空数组）
 * @param {AbortController} abortCtrl - 可选的中断控制器，用于手动取消上传（如用户点击取消）
 * @returns {Promise} - 所有分片上传完成的Promise
 */
const uploadChunks = async (chunks, fileHash, existChunks = [], abortCtrl = null) => {
  // 已上传分片数：初始值为后端返回的已传分片数量（断点续传的起点）
  let uploadedCount = existChunks.length;
  // 总分片数：用于计算上传进度
  const totalChunks = chunks.length;
  
  // 初始化上传进度（已传分片数 / 总分片数 * 100，取整）
  uploadProgress.value = Math.round((uploadedCount / totalChunks) * 100);
  
  // 生成分片上传任务列表：遍历所有分片，为每个分片创建上传Promise
  const uploadTasks = chunks.map((chunk, index) => {
    // 分片标识：用分片索引作为唯一标识（生产环境建议拼接文件哈希，避免冲突）
    const chunkHash = `${index}`;
    
    // 断点续传核心：如果该分片已上传过，直接返回成功的Promise，跳过上传
    if (existChunks.includes(chunkHash)) {
      return Promise.resolve();
    }
    
    // 构建FormData：用于传递二进制分片数据（后端multiparty需解析该格式）
    const formData = new FormData();
    formData.append('chunk', chunk);        // 分片二进制数据
    formData.append('filehash', fileHash);  // 文件整体哈希（后端用于创建分片存储目录）
    formData.append('chunkhash', chunkHash); // 分片标识（后端用于命名分片文件）
    
    // 发送分片上传请求
    return fetch('http://localhost:3000/upload', {
      method: 'POST',                // POST请求（必须，用于传递FormData）
      body: formData,                // 请求体：包含分片和标识的FormData
      signal: abortCtrl?.signal      // 绑定中断信号：支持手动取消上传
    })
    // 解析后端返回的JSON数据
    .then(response => response.json())
    .then(result => {
      // 分片上传失败：抛出错误，终止Promise.all（避免部分分片上传成功）
      if (!result.status) {
        throw new Error(`分片 ${index} 上传失败`);
      }
      // 分片上传成功：更新已传数量 + 实时刷新上传进度
      uploadedCount++;
      uploadProgress.value = Math.round((uploadedCount / totalChunks) * 100);
    });
  });
  
  // 并行执行所有分片上传任务：等待所有分片上传完成（失败则整体失败）
  await Promise.all(uploadTasks);
};
```


### 问题汇总？
#### 1.请问如何做的进度显示？
在这个项目中有一步是文件校验，目的就是看一下服务器上的文件是否已经存在？文件分片哪些存在？所以进度显示=已上传的文件片/总的文件片。
在对应的后端接口返回`{ shouldUpload: true, existChunks: [已上传分片列表] }`,能得到已上传的文件片数量 = `existChunks.length`。

#### 2.上传文件为什么不能用 JSON？FormData 底层传输格式是什么
1.为什么不能用 JSON 传文件：**JSON 是文本格式**，仅支持字符串、数字、布尔等基础类型，**无法直接表示二进制文件数据**（如图片、视频的字节流）；强行转换二进制会导致数据丢失或体积膨胀。
2.FormData 的作用：**专门用于构建包含二进制数据的请求体**，支持文件、字符串等混合数据传输，适配 HTTP 文件上传的规范。
3.FormData 底层传输格式：遵循 multipart/form-data 格式，这是 HTTP 协议中专门用于传输二进制文件的请求体格式。

#### 3.上传文件时，File对象和Blob对象的区别是什么？分片上传时为什么常用 Blob.slice() 而不是直接用 File？
1.**File vs Blob 核心区别**：Blob 是二进制数据的基础容器（仅包含数据和类型），File 是继承自 Blob 的特殊类型，额外包含文件的元信息（文件名、修改时间、大小等）；
slice() 是 Blob 的原生方法，File 仅继承使用。使用`Blob.slice()`更加本质，能处理更多的二进制数据。

::: details
- **第一轮：基础概念拷打（必问，考察底层认知）**

前端上传文件时，为什么不能用 JSON 格式传递文件，必须用 FormData？FormData 的底层传输格式是什么？
`<input type="file"> `的 accept 属性能完全限制文件类型吗？为什么？怎么从根本上保证文件类型合规？
文件夹上传的 `<input type="file" webkitdirectory>` 有兼容性问题吗？如果要兼容低版本浏览器，你有什么替代方案？
上传文件时，File 对象和 Blob 对象的区别是什么？分片上传时为什么常用 Blob.slice() 而不是直接用 File？
说说文件上传的 HTTP 响应码：200、400、413、500 分别对应上传的什么问题？前端该怎么处理这些状态码？

- **第二轮：分片上传核心拷打（重点，考察实战能力）**

你写的分片上传代码里用了 Promise.all 并行上传，假如有 1000 个分片，直接用 Promise.all 会有什么问题？怎么限制并发数（比如最多同时传 6 个）？
分片上传的「文件哈希」怎么计算？如果是 10GB 的大文件，直接计算整个文件的 MD5 会有什么问题？怎么优化（结合你之前写的采样逻辑）？
断点续传中，后端是怎么记录 “已上传分片” 的？如果后端重启，已上传的分片记录丢了怎么办？
你的分片合并逻辑里，前端怎么确保所有分片都上传完成了才调用 /merge 接口？如果少传了一个分片就合并，会出现什么问题？
分片上传时，某个分片上传失败了，你会怎么处理？只重试一次够吗？重试的策略（比如指数退避）该怎么设计？

- **第三轮：文件夹上传 + Web Worker 拷打（进阶，考察性能优化）**

为什么大文件 / 文件夹上传必须用 `Web Worker`？`Web Worker` 里能直接发 fetch 请求上传分片吗？为什么？

`Web Worker` 和主线程之间传递大文件 / 分片时，会发生数据拷贝吗？怎么用 `Transferable Objects` 优化内存占用？

遍历多层嵌套文件夹时，前端怎么递归读取所有文件（包括子文件夹里的文件）？如果文件夹里有 1 万个小文件，递归会导致栈溢出吗？怎么解决？

文件夹上传时，怎么保持文件的「目录结构」（比如 /docs/前端/面试.md）？后端该怎么存储这个结构，下载时又怎么还原？

`Web Worker `能访问 localStorage 吗？如果要在 Worker 里缓存已计算的文件哈希，你有什么方案？

- **第四轮：异常处理 & 工程化拷打（深度，考察稳定性）**

上传过程中用户刷新页面，断点续传的进度丢了怎么办？前端该怎么持久化进度（比如 localStorage/indexedDB）？

上传大文件时，浏览器提示「请求超时」，你会怎么优化？（从前端超时配置、后端超时配置、请求重连三个角度说）

上传文件时，用户切换网络（比如从 WiFi 切到 4G），前端该怎么感知网络变化？怎么暂停 / 恢复上传？

你的代码里用了 abortCtrl 中断上传，中断后已上传的分片该怎么处理？前端主动中断和网络异常中断的处理逻辑有区别吗？

生产环境中，怎么监控文件上传的成功率？如果某个用户上传失败率高，你会从哪些维度排查问题（前端 / 网络 / 后端）？

- **第五轮：性能 & 安全拷打（大厂高频，考察综合能力）**

分片上传的「分片大小」怎么定？太小（比如 100KB）或太大（比如 100MB）各有什么问题？结合网络环境（弱网 / 千兆网）怎么动态调整？

怎么防止「恶意分片攻击」？比如有人故意上传大量无效分片，占满服务器磁盘？前端能做什么限制？

上传进度条的「伪进度」问题：比如前 90% 进度走得快，最后 10% 卡很久，怎么优化进度计算的准确性？

秒传功能的核心是文件哈希，但是计算哈希本身也耗时，怎么在用户选择文件后 “提前预计算” 哈希，优化体验？

跨域上传文件时，为什么会先触发 OPTIONS 预检请求？怎么让后端配合优化预检请求的开销？

- **附加轮：场景化拷打（考察落地能力）**

产品要求：上传 10GB 的视频文件，支持「暂停 / 继续」「断点续传」「上传速度显示」「预估剩余时间」，你会怎么设计整体流程？

对比「普通上传」「分片上传」「断点续传」「秒传」的适用场景，分别举 1 个业务案例说明该用哪种？

如果后端不支持分片上传，你怎么在前端实现 “伪断点续传”？（比如基于 Range 请求）

上传后的文件需要前端预览（比如图片 / 视频 / 文档），怎么实现？大视频预览怎么优化（比如只加载首帧）？
:::
### Node后端部分
在文件上传这一部分功能主要使用的几个依赖包是:
`express`（node框架，在这里主要用于简单的http请求）、`fs-extra`（代替node内置fs,主要用于处理文件的写入、读取、删除等）、`multiparty`（用于处理前端传过来的formData参数等）、`nodemon`（用于自动重启Node服务器及响应代码更改，不然你改一次代码就得重新启动一次）、cors处理跨域问题。
```js
const express = require('express');
const multiparty = require('multiparty');
const fs = require('fs-extra');
const cors = require('cors');
const path = require('path');

const app = express();
const PORT = process.env.PORT || 3000;
const uploadDir = path.join(__dirname, 'uploads');

// 初始化：创建上传目录 + 配置中间件
fs.ensureDirSync(uploadDir);
app.use(cors());
app.use(express.json());
app.use(express.urlencoded({ extended: true }));

// 工具函数：提取文件扩展名
const getExt = (fileName) => fileName.slice(fileName.lastIndexOf('.'));

// 工具函数：管道流合并分片
const pipeStream = (readPath, writeStream) => new Promise(resolve => {
  const rs = fs.createReadStream(readPath);
  rs.on('end', resolve);
  rs.pipe(writeStream);
});

// 1. 文件校验接口（秒传/断点续传）
app.post('/verify', async (req, res) => {
  try {
    const { fileHash, fileName } = req.body;
    const fullPath = path.resolve(uploadDir, `${fileHash}${getExt(fileName)}`);
    
    // 秒传：文件已存在则无需上传
    if (fs.existsSync(fullPath)) {
      return res.json({ shouldUpload: false });
    }
    
    // 断点续传：返回已上传的分片列表
    const chunkDir = path.resolve(uploadDir, fileHash);
    const existChunks = fs.existsSync(chunkDir) ? await fs.readdir(chunkDir) : [];
    res.json({ shouldUpload: true, existChunks });
  } catch (err) {
    res.status(500).json({ error: '校验失败', msg: err.message });
  }
});

// 2. 分片上传接口
app.post('/upload', (req, res) => {
  const form = new multiparty.Form();
  form.parse(req, async (err, fields, files) => {
    if (err) {
      return res.status(500).json({ error: '分片上传失败', msg: err.message });
    }

    const [fileHash, chunkHash] = [fields.filehash[0], fields.chunkhash[0]];
    const chunkDir = path.resolve(uploadDir, fileHash);
    
    await fs.ensureDir(chunkDir);
    await fs.move(files.chunk[0].path, path.resolve(chunkDir, chunkHash));
    res.json({ status: true });
  });
});

// 3. 分片合并接口
app.post('/merge', async (req, res) => {
  try {
    const { fileHash, fileName, size } = req.body;
    const fullPath = path.resolve(uploadDir, `${fileHash}${getExt(fileName)}`);
    const chunkDir = path.resolve(uploadDir, fileHash);

    // 获取并排序分片文件
    const chunkFiles = await fs.readdir(chunkDir);
    chunkFiles.sort((a, b) => parseInt(a.split('-')[1]) - parseInt(b.split('-')[1]));

    // 合并分片
    for (let i = 0; i < chunkFiles.length; i++) {
      const chunkPath = path.resolve(chunkDir, chunkFiles[i]);
      await pipeStream(chunkPath, fs.createWriteStream(fullPath, { start: i * size }));
      await fs.unlink(chunkPath); // 删除已合并的分片
    }

    await fs.remove(chunkDir); // 删除分片目录
    res.json({ status: true, msg: '合并完成' });
  } catch (err) {
    res.status(500).json({ error: '合并失败', msg: err.message });
  }
});

// 启动服务
app.listen(PORT, () => {
  console.log(`服务器运行在 http://localhost:${PORT}`);
  console.log('接口列表：/verify(POST)、/upload(POST)、/merge(POST)');
});
```
## 6. Web Worker



